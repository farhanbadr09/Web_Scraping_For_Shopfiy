from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

# Initialize Chrome WebDriver with options
options = webdriver.ChromeOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--ignore-ssl-errors')
driver = webdriver.Chrome(options=options)

# Initialize a list to store product names
product_names = []

# Open the main page
driver.get("https://wholesale.stylekorean.com/Product/BestProduct")

try:
    # Wait for a longer period or check if any product item is available
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.CLASS_NAME, 'productTxt'))  # Waiting for product names to load
    )
    time.sleep(3)  # Allow the page to load

    # Find the total number of pages
    pagination = driver.find_elements(By.CLASS_NAME, 'page-link')
    total_pages = max([int(btn.get_attribute('page')) for btn in pagination if btn.get_attribute('page').isdigit()])

    # Loop through pages
    for page in range(1, total_pages + 1):
        print(f"Scraping page {page}...")

        # Click the button corresponding to the current page
        page_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, f"//a[@class='page-link' and @page='{page}']"))
        )
        page_button.click()
        time.sleep(3)  # Allow the page to load

        # Parse the current page
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # Find the container that holds the product list
        product_list = soup.find('div', class_='row', id='p-list')

        if product_list:
            # Find all product name elements within the list
            product_elements = product_list.find_all('span', class_='productTxt')

            # Extract and store product names
            for product in product_elements:
                product_name = product.text.strip()
                product_names.append(product_name)

finally:
    # Close the WebDriver
    driver.quit()

# Print the product names
print(f"Scraped {len(product_names)} product names:")
for name in product_names:
    print(name)
